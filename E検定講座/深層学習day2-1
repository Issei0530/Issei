import sys, os
sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定
import numpy as np
from common import layers
from collections import OrderedDict
from common import functions
from data.mnist import load_mnist
import matplotlib.pyplot as plt


class MultiLayerNet:
    '''
    input_size: 入力層のノード数
    hidden_size_list: 隠れ層のノード数のリスト
    output_size: 出力層のノード数
    activation: 活性化関数
    weight_init_std: 重みの初期化方法
    '''
    def __init__(self, input_size, hidden_size_list, output_size, activation='relu', weight_init_std='relu'):
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_size_list = hidden_size_list
        self.hidden_layer_num = len(hidden_size_list)
        self.params = {}

        # 重みの初期化
        self.__init_weight(weight_init_std)

        # レイヤの生成, sigmoidとreluのみ扱う
        activation_layer = {'sigmoid': layers.Sigmoid, 'relu': layers.Relu}
        self.layers = OrderedDict() # 追加した順番に格納
        for idx in range(1, self.hidden_layer_num+1):
            self.layers['Affine' + str(idx)] = layers.Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])
            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()

        idx = self.hidden_layer_num + 1
        self.layers['Affine' + str(idx)] = layers.Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])

        self.last_layer = layers.SoftmaxWithLoss()

    def __init_weight(self, weight_init_std):
        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]
        for idx in range(1, len(all_size_list)):
            scale = weight_init_std
            if str(weight_init_std).lower() in ('relu', 'he'):
                scale = np.sqrt(2.0 / all_size_list[idx - 1])
            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):
                scale = np.sqrt(1.0 / all_size_list[idx - 1])

            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])
            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])

    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)

        return x

    def loss(self, x, d):
        y = self.predict(x)
        
        weight_decay = 0
        for idx in range(1, self.hidden_layer_num + 2):
            W = self.params['W' + str(idx)]
        
        return self.last_layer.forward(y, d) + weight_decay
        
    
    def accuracy(self, x, d):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if d.ndim != 1 : d = np.argmax(d, axis=1)

        accuracy = np.sum(y == d) / float(x.shape[0])
        return accuracy

    def gradient(self, x, d):
        # forward
        self.loss(x, d)
        
        # backward
        dout = 1
        dout = self.last_layer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)

        
        # 設
        grad = {}
        for idx in range(1, self.hidden_layer_num+2):
            grad['W' + str(idx)] = self.layers['Affine' + str(idx)].dW
            grad['b' + str(idx)] = self.layers['Affine' + str(idx)].db

        return grad

# データの読み込み
(x_train, d_train), (x_test, d_test) = load_mnist(normalize=True, one_hot_label=True)

iters_num = 2000
train_size = x_train.shape[0]
batch_size = 100

plot_interval=10

activator_initializer_list = (['sigmoid', 'gauss'],
                              ['sigmoid', 'Xavier'],
                              ['sigmoid', 'He'],
                              ['ReLU', 'gauss'],
                              ['ReLU', 'Xavier'],
                              ['ReLU', 'He']
                              )
learning_rate_list = (0.01, 0.1, 1)

# まとめて処理

# グラフを並べる個数
fig_row = 2
fig_col = 3

for learning_rate in learning_rate_list:
    print('学習率が{}の場合の挙動'.format(learning_rate))

    # learning rateごとに図を分ける
    fig, axes = plt.subplots(fig_row, fig_col, figsize=(12,8), sharex=True, sharey=True)
    axes[1, 0].set_xlabel("iteration count")
    axes[1, 1].set_xlabel("iteration count")
    axes[1, 2].set_xlabel("iteration count")
    axes[0, 0].set_ylabel("accuracy")
    axes[0, 1].set_ylabel("accuracy")
    axes[0, 0].set_ylim(0, 1.0)

    
    axes_counter = 0    
    
    # activatorとinitializerごとに学習を実施
    for activator, initializer in activator_initializer_list:

        # activatorとinitializerをMultiLayerNetに代入できる形に指定
        if activator == 'sigmoid':
            activation = 'sigmoid'
        elif activator == 'ReLU':
            activation = 'relu'
        else:
            raise ValueError('Activator is not supported.')

        if initializer == 'gauss':
            weight_init_std = 0.01
        elif initializer == 'Xavier':
            weight_init_std = 'Xavier'
        elif initializer == 'He':
            weight_init_std = 'He'
        else:
            raise ValueError('Initializer is not supported.')

        # ここから学習
        train_loss_list = []
        accuracies_train = []
        accuracies_test = []
        network = MultiLayerNet(input_size=784, hidden_size_list=[40, 20], output_size=10, 
                                activation=activation, weight_init_std=weight_init_std)

        for i in range(iters_num):
            batch_mask = np.random.choice(train_size, batch_size)
            x_batch = x_train[batch_mask]
            d_batch = d_train[batch_mask]

            # 勾配
            grad = network.gradient(x_batch, d_batch)
            for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):
                network.params[key] -= learning_rate * grad[key]
            loss = network.loss(x_batch, d_batch)
            train_loss_list.append(loss)

            if (i + 1) % plot_interval == 0:
                accr_test = network.accuracy(x_test, d_test)
                accuracies_test.append(accr_test)        
                accr_train = network.accuracy(x_batch, d_batch)
                accuracies_train.append(accr_train)

        # 可視化
        axes_row_index = axes_counter // fig_col
        axes_col_index = axes_counter % fig_col
            
        lists = range(0, iters_num, plot_interval)
        axes[axes_row_index, axes_col_index].plot(lists, accuracies_train, label="training set")
        axes[axes_row_index, axes_col_index].plot(lists, accuracies_test,  label="test set")
        axes[axes_row_index, axes_col_index].legend()
        axes[axes_row_index, axes_col_index].set_title('{str1} + {str2}'.format(str1=activator, str2=initializer))
        axes_counter += 1


    # グラフの表示
    fig.tight_layout()
    plt.show()



# 中間層の変更
# まとめて処理

# グラフを並べる個数
fig_row = 2
fig_col = 3

for learning_rate in learning_rate_list:
    print('学習率が{}の場合の挙動'.format(learning_rate))

    # learning rateごとに図を分ける
    fig, axes = plt.subplots(fig_row, fig_col, figsize=(12,8), sharex=True, sharey=True)
    axes[1, 0].set_xlabel("iteration count")
    axes[1, 1].set_xlabel("iteration count")
    axes[1, 2].set_xlabel("iteration count")
    axes[0, 0].set_ylabel("accuracy")
    axes[0, 1].set_ylabel("accuracy")
    axes[0, 0].set_ylim(0, 1.0)

    
    axes_counter = 0    
    
    # activatorとinitializerごとに学習を実施
    for activator, initializer in activator_initializer_list:

        # activatorとinitializerをMultiLayerNetに代入できる形に指定
        if activator == 'sigmoid':
            activation = 'sigmoid'
        elif activator == 'ReLU':
            activation = 'relu'
        else:
            raise ValueError('Activator is not supported.')

        if initializer == 'gauss':
            weight_init_std = 0.01
        elif initializer == 'Xavier':
            weight_init_std = 'Xavier'
        elif initializer == 'He':
            weight_init_std = 'He'
        else:
            raise ValueError('Initializer is not supported.')

        # ここから学習
        train_loss_list = []
        accuracies_train = []
        accuracies_test = []
        network = MultiLayerNet(input_size=784, hidden_size_list=[40, 20, 20, 20, 20, 20, 20, 20], output_size=10, 
                                activation=activation, weight_init_std=weight_init_std)

        for i in range(iters_num):
            batch_mask = np.random.choice(train_size, batch_size)
            x_batch = x_train[batch_mask]
            d_batch = d_train[batch_mask]

            # 勾配
            grad = network.gradient(x_batch, d_batch)
            for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):
                network.params[key] -= learning_rate * grad[key]
            loss = network.loss(x_batch, d_batch)
            train_loss_list.append(loss)

            if (i + 1) % plot_interval == 0:
                accr_test = network.accuracy(x_test, d_test)
                accuracies_test.append(accr_test)        
                accr_train = network.accuracy(x_batch, d_batch)
                accuracies_train.append(accr_train)

        # 可視化
        axes_row_index = axes_counter // fig_col
        axes_col_index = axes_counter % fig_col
            
        lists = range(0, iters_num, plot_interval)
        axes[axes_row_index, axes_col_index].plot(lists, accuracies_train, label="training set")
        axes[axes_row_index, axes_col_index].plot(lists, accuracies_test,  label="test set")
        axes[axes_row_index, axes_col_index].legend()
        axes[axes_row_index, axes_col_index].set_title('{str1} + {str2}'.format(str1=activator, str2=initializer))
        axes_counter += 1


    # グラフの表示
    fig.tight_layout()
    plt.show()

