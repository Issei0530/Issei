import sys, os
sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定
import numpy as np
from collections import OrderedDict
from common import layers
from data.mnist import load_mnist
import matplotlib.pyplot as plt
from multi_layer_net import MultiLayerNet

# データの読み込み
(x_train, d_train), (x_test, d_test) = load_mnist(normalize=True, one_hot_label=True)

# ハイパーパラメータ設定
iters_num = 1000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.01
momentum = 0.9
decay_rate = 0.99
beta1 = 0.9
beta2 = 0.999

plot_interval=10
optimizer_list=['SGD', 'Momentum', 'AdaGrad', 'RMSProp', 'Adam']


# グラフ表示用の設定
fig_row = 2
fig_col = 3

# 1回目はbatchnormなし、2回目はbatchnormありの繰り返し
for i in range(2):
    axes_counter = 0
    if i == 0:
        use_batchnorm = False
        print('batch正規化なしの場合')
    elif i == 1:
        use_batchnorm = True
        print('batch正規化ありの場合')
    
    fig, axes = plt.subplots(fig_row, fig_col, figsize=(12,8), sharex=True, sharey=True)
    axes[1, 0].set_xlabel("iteration count")
    axes[1, 1].set_xlabel("iteration count")
    axes[1, 2].set_xlabel("iteration count")
    axes[0, 0].set_ylabel("accuracy")
    axes[0, 1].set_ylabel("accuracy")
    axes[0, 0].set_xlim(0, iters_num)
    axes[0, 0].set_ylim(0, 1.0)
                
    # 重みの更新方法を変えての繰り返し
    for optimizer in optimizer_list:
        train_loss_list = []
        accuracies_train = []
        accuracies_test = []

        network = MultiLayerNet(input_size=784, hidden_size_list=[40, 20], output_size=10, activation='sigmoid',
                                weight_init_std=0.01, use_batchnorm=use_batchnorm)
        
        for i in range(iters_num):
            batch_mask = np.random.choice(train_size, batch_size)
            x_batch = x_train[batch_mask]
            d_batch = d_train[batch_mask]

            if optimizer == 'SGD':
                grad = network.gradient(x_batch, d_batch)
                for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):
                    network.params[key] -= learning_rate * grad[key]

                    loss = network.loss(x_batch, d_batch)
                    train_loss_list.append(loss)
                    
            if optimizer == 'Momentum':
                grad = network.gradient(x_batch, d_batch)
                if i == 0:
                    v = {}
                for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):
                    if i == 0:
                        v[key] = np.zeros_like(network.params[key])
                    v[key] = momentum * v[key] - learning_rate * grad[key]
                    network.params[key] += v[key]

                    loss = network.loss(x_batch, d_batch)
                    train_loss_list.append(loss)

            if optimizer == 'AdaGrad':
                grad = network.gradient(x_batch, d_batch)
                if i == 0:
                    h = {}
                for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):
                    if i == 0:
                        h[key] = np.full_like(network.params[key], 1e-4)
                    else:
                        h[key] += np.square(grad[key])
                    network.params[key] -= learning_rate * grad[key] / (np.sqrt(h[key]))

                    loss = network.loss(x_batch, d_batch)
                    train_loss_list.append(loss)                        
                                
            if optimizer == 'RMSProp':
                grad = network.gradient(x_batch, d_batch)
                if i == 0:
                    h = {}
                for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):
                    if i == 0:
                        h[key] = np.zeros_like(network.params[key])
                    h[key] *= decay_rate
                    h[key] += (1 - decay_rate) * np.square(grad[key])
                    network.params[key] -= learning_rate * grad[key] / (np.sqrt(h[key]) + 1e-7)

                    loss = network.loss(x_batch, d_batch)
                    train_loss_list.append(loss)                                
                
            if optimizer == 'Adam':
                grad = network.gradient(x_batch, d_batch)
                if i == 0:
                    m = {}
                    v = {}
                learning_rate_t  = learning_rate * np.sqrt(1.0 - beta2 ** (i + 1)) / (1.0 - beta1 ** (i + 1))    
                for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):
                    if i == 0:
                        m[key] = np.zeros_like(network.params[key])
                        v[key] = np.zeros_like(network.params[key])

                    m[key] += (1 - beta1) * (grad[key] - m[key])
                    v[key] += (1 - beta2) * (grad[key] ** 2 - v[key])            
                    network.params[key] -= learning_rate_t * m[key] / (np.sqrt(v[key]) + 1e-7)                

                    loss = network.loss(x_batch, d_batch)
                    train_loss_list.append(loss)      

            if (i + 1) % plot_interval == 0:
                accr_test = network.accuracy(x_test, d_test)
                accuracies_test.append(accr_test)        
                accr_train = network.accuracy(x_batch, d_batch)
                accuracies_train.append(accr_train)
                    
        # 可視化
        axes_row_index = axes_counter // fig_col
        axes_col_index = axes_counter % fig_col
            
        lists = range(0, iters_num, plot_interval)
        axes[axes_row_index, axes_col_index].plot(lists, accuracies_train, label="training set")
        axes[axes_row_index, axes_col_index].plot(lists, accuracies_test,  label="test set")
        axes[axes_row_index, axes_col_index].set_title(optimizer)
        axes[axes_row_index, axes_col_index].legend()
        axes_counter += 1

    # グラフの表示
    fig.tight_layout()
    plt.show()
