import sys, os
sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定
import numpy as np
from collections import OrderedDict
from common import layers
from data.mnist import load_mnist
import matplotlib.pyplot as plt
from multi_layer_net import MultiLayerNet
from common import optimizer

(x_train, d_train), (x_test, d_test) = load_mnist(normalize=True)

# 過学習を再現するために、学習データを削減
x_train = x_train[:300]
d_train = d_train[:300]

learning_rate=0.01
use_dropout = True
optimizer = optimizer.SGD(learning_rate=learning_rate)　# SGDを用いる場合
optimizer = optimizer.Adam()  # Adamを用いる場合

iters_num = 1000
train_size = x_train.shape[0]
batch_size = 100

train_loss_list = []
accuracies_train = []
accuracies_test = []

plot_interval=10
hidden_layer_num = network.hidden_layer_num

# 正則化方法のリストを作成（正則化方法／正則化の強さ）
# 正則化の強さが0の場合は、正則化していないことに相当
norm_condition = (['L2', 0.05],
                  ['L2', 0.25],
                  ['L2', 1],
                  ['L1', 0.05],
                  ['L1', 0.25],
                  ['L1', 1],
                  ['L2', 0])

dropout_condition = ([0, 0.1, 0.5])


class Dropout:
    def __init__(self, dropout_ratio=0.5):
        self.dropout_ratio = dropout_ratio
        self.mask = None

    def forward(self, x, train_flg=True):
        if train_flg:
            self.mask = np.random.rand(*x.shape) > self.dropout_ratio
            return x * self.mask
        else:
            return x * (1.0 - self.dropout_ratio)

    def backward(self, dout):
        return dout * self.mask
        


# まとめて処理

# グラフを並べる個数
fig_row = 3
fig_col = 3

for dropout_ratio in dropout_condition:
    print(f'dropout率が{dropout_ratio}の場合')
    
    network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,
                            weight_decay_lambda=weight_decay_lambda, use_dropout = use_dropout, dropout_ratio = dropout_ratio)

    fig, axes = plt.subplots(fig_row, fig_col, figsize=(12,8), sharex=True, sharey=True)
    axes[1, 0].set_xlabel("iteration count")
    axes[1, 1].set_xlabel("iteration count")
    axes[1, 2].set_xlabel("iteration count")
    axes[0, 0].set_ylabel("accuracy")
    axes[0, 1].set_ylabel("accuracy")
    axes[0, 0].set_ylim(0, 1.0)
    
    for norm_method, weight_decay_lambda in norm_condition:

        axes_counter = 0    

        train_loss_list = []
        accuracies_train = []
        accuracies_test = []    

        for i in range(iters_num):
            batch_mask = np.random.choice(train_size, batch_size)
            x_batch = x_train[batch_mask]
            d_batch = d_train[batch_mask]

            grad = network.gradient(x_batch, d_batch)
            weight_decay = 0

            if norm_method == 'L2':
                for idx in range(1, hidden_layer_num+1):
                    grad['W' + str(idx)] = network.layers['Affine' + str(idx)].dW + weight_decay_lambda * network.params['W' + str(idx)]
                    grad['b' + str(idx)] = network.layers['Affine' + str(idx)].db
                    network.params['W' + str(idx)] -= learning_rate * grad['W' + str(idx)]
                    network.params['b' + str(idx)] -= learning_rate * grad['b' + str(idx)]        
                    weight_decay += 0.5 * weight_decay_lambda * np.sqrt(np.sum(network.params['W' + str(idx)] ** 2))

            elif norm_method == 'L1':
                for idx in range(1, hidden_layer_num+1):
                    grad['W' + str(idx)] = network.layers['Affine' + str(idx)].dW + weight_decay_lambda * network.params['W' + str(idx)]
                    grad['b' + str(idx)] = network.layers['Affine' + str(idx)].db
                    network.params['W' + str(idx)] -= learning_rate * grad['W' + str(idx)]
                    network.params['b' + str(idx)] -= learning_rate * grad['b' + str(idx)]        
                    weight_decay += 0.5 * weight_decay_lambda * np.sqrt(np.sum(network.params['W' + str(idx)] ** 2))

            else:
                raise ValueError('norm_conditionの正則化方法はL1かL2で設定してください。')

            loss = network.loss(x_batch, d_batch) + weight_decay
            train_loss_list.append(loss)        

            if (i+1) % plot_interval == 0:
                accr_train = network.accuracy(x_train, d_train)
                accr_test = network.accuracy(x_test, d_test)
                accuracies_train.append(accr_train)
                accuracies_test.append(accr_test)


        # 可視化
        axes_row_index = axes_counter // fig_col
        axes_col_index = axes_counter % fig_col

        lists = range(0, iters_num, plot_interval)
        axes[axes_row_index, axes_col_index].plot(lists, accuracies_train, label="training set")
        axes[axes_row_index, axes_col_index].plot(lists, accuracies_test,  label="test set")
        axes[axes_row_index, axes_col_index].legend()
        axes[axes_row_index, axes_col_index].set_title(f"method:{norm_method} / norm_weight:{weight_decay_lambda}")
        axes_counter += 1

    # グラフの表示
    fig.tight_layout()
    plt.show()
